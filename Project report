Project Submission ReportProject Title: Running GenAI on Intel AI Laptops and Simple LLM Inference on CPU with Fine-Tuning of LLM Models using Intel® OpenVINO™Problem StatementThe objective of this project is to leverage Intel AI laptops to run Generative AI (GenAI) applications and perform simple large language model (LLM) inference on CPUs. Additionally, the project aims to fine-tune LLM models using Intel® OpenVINO™, optimizing the performance and deployment of these models on Intel hardware.Unique Idea Brief (Solution)The proposed solution involves using Intel® OpenVINO™ to optimize and accelerate LLMs for efficient inference on Intel CPUs and AI laptops. By converting models to the Intermediate Representation (IR) format, we can leverage OpenVINO™'s optimization capabilities to enhance performance. The solution also includes a streamlined process for fine-tuning LLMs using popular frameworks like PyTorch, followed by optimization for deployment on Intel hardware.Features OfferedModel Conversion and Optimization: Convert pre-trained and fine-tuned LLM models to the Intermediate Representation (IR) format using OpenVINO™ Model Optimizer.Efficient Inference: Perform high-performance inference on Intel CPUs and integrated GPUs using the OpenVINO™ Inference Engine.Fine-Tuning Support: Fine-tune LLM models using frameworks like PyTorch or TensorFlow, and optimize them for efficient inference.Comprehensive Documentation: Detailed guides and examples for model conversion, optimization, and inference.Scalable Deployment: Best practices for deploying optimized models on various Intel AI hardware configurations.Process FlowModel Preparation: Obtain pre-trained LLM models (e.g., GPT, BERT).Model Conversion: Convert the models to ONNX format if necessary.Optimization: Use OpenVINO™ Model Optimizer to convert ONNX models to the IR format.Inference Execution: Run inference using the OpenVINO™ Inference Engine on Intel CPUs and integrated GPUs.Fine-Tuning: Fine-tune models using PyTorch or TensorFlow.Re-Optimization: Optimize fine-tuned models for efficient deployment using OpenVINO™.Deployment: Deploy optimized models on Intel AI laptops and other Intel hardware.
